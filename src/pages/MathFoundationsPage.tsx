import React from 'react';
import SectionHeading from '../components/SectionHeading';
import ContentBlock from '../components/ContentBlock';
import MathEquation from '../components/MathEquation';
import CodeBlock from '../components/CodeBlock';
import LinearAlgebraSection from '../components/LinearAlgebraSection';

const MathFoundationsPage: React.FC = () => {
  const handleOpenCalculusGuide = () => {
    window.open('/Module_1_calculus.pdf', '_blank');
  };

  const handleOpenLinearAlgebraGuide = () => {
    window.open('/Module_2_linear_algebra.pdf', '_blank');
  };

  const gradientDescentCode = `
import numpy as np
import matplotlib.pyplot as plt

def gradient_descent(f, df, x0, learning_rate=0.1, num_iterations=100):
    """
    Simple gradient descent implementation
    
    Args:
        f: Function to minimize
        df: Gradient of f
        x0: Initial point (numpy array)
        learning_rate: Step size alpha
        num_iterations: Number of iterations
        
    Returns:
        x_history: History of x values during optimization
        f_history: History of function values
    """
    x = x0.copy()
    x_history = [x.copy()]
    f_history = [f(x)]
    
    for i in range(num_iterations):
        # Compute gradient
        gradient = df(x)
        
        # Update x by taking a step in the negative gradient direction
        x = x - learning_rate * gradient
        
        # Store x and f(x)
        x_history.append(x.copy())
        f_history.append(f(x))
        
    return np.array(x_history), np.array(f_history)

# Example usage:
# Define a simple quadratic function and its gradient
def f(x):
    return x[0]**2 + 2*x[1]**2

def df(x):
    return np.array([2*x[0], 4*x[1]])

# Run gradient descent
x0 = np.array([3.0, -2.0])
x_history, f_history = gradient_descent(f, df, x0)

print(f"Starting point: {x0}")
print(f"Final point: {x_history[-1]}")
print(f"Function value at minimum: {f_history[-1]}")
`;

  return (
    <div>
      {/* Page Header */}
      <section className="bg-gradient-to-r from-blue-800 to-indigo-700 py-16 px-4">
        <div className="container mx-auto">
          <div className="max-w-3xl">
            <h1 className="text-4xl font-bold text-white mb-4">Mathematical Foundations</h1>
            <p className="text-xl text-blue-100">
              The essential mathematical concepts underlying reinforcement learning theory and algorithms.
            </p>
          </div>
        </div>
      </section>

      <div className="container mx-auto px-4 py-12">
        {/* Introduction */}
        <SectionHeading 
          title="Mathematical Foundations of Reinforcement Learning" 
          subtitle="Understanding the theoretical framework that makes reinforcement learning possible."
        />

        <ContentBlock>
          <p className="text-lg leading-relaxed mb-4">
            Reinforcement learning is built upon a rich mathematical foundation that includes probability theory,
            Markov decision processes, dynamic programming, and optimization. These mathematical tools provide
            the theoretical framework for understanding how agents can learn optimal behavior through interaction
            with an environment.
          </p>
          <p className="mb-4">
            In this section, we'll explore the key mathematical concepts that form the backbone of reinforcement
            learning algorithms and theory. Understanding these foundations is essential for grasping how and why
            reinforcement learning algorithms work.
          </p>
        </ContentBlock>

        {/* Linear Algebra */}
        <LinearAlgebraSection 
          onOpenGuide={handleOpenLinearAlgebraGuide}
          equation="V = Œ¶w"
          description="Linear function approximation of value function V using feature matrix Œ¶ and weight vector w"
        />

        {/* Calculus */}
        <SectionHeading 
          title="Calculus" 
          subtitle="The fundamental mathematical framework for understanding change and optimization."
        />

        <div className="bg-blue-700 rounded-lg overflow-hidden shadow-xl mb-8">
          <div className="p-6">
            <h2 className="text-2xl font-bold text-white mb-2">Calculus</h2>
            <p className="text-blue-100 mb-6">
              The fundamental mathematical framework for understanding change and optimization.
            </p>

            <button
              onClick={handleOpenCalculusGuide}
              className="block w-full p-6 bg-blue-600 rounded-lg hover:bg-blue-500 transition-colors duration-300 mb-8 text-left cursor-pointer"
            >
              <h3 className="text-xl font-semibold mb-2 text-white">üìö Calculus in Reinforcement Learning</h3>
              <p className="text-blue-100">
                Click to open the comprehensive guide on calculus fundamentals and their applications in reinforcement learning.
                This document covers derivatives, integrals, and their role in optimization algorithms.
              </p>
            </button>

            <div className="space-y-6 text-white">
              <h3 className="text-xl font-semibold mb-4">Key Concepts in Calculus</h3>
              <ul className="list-disc pl-6 space-y-2 text-blue-100">
                <li><strong className="text-white">Derivatives:</strong> Measuring rates of change and finding optimal solutions</li>
                <li><strong className="text-white">Integrals:</strong> Accumulating continuous quantities over time or space</li>
                <li><strong className="text-white">Multivariate Calculus:</strong> Working with functions of multiple variables</li>
                <li><strong className="text-white">Optimization:</strong> Finding maxima and minima of functions</li>
              </ul>

              <div className="bg-blue-600 p-6 rounded-lg mt-8">
                <h4 className="font-semibold mb-3 text-white">Applications in Reinforcement Learning:</h4>
                <ul className="space-y-2 text-blue-100">
                  <li>‚Ä¢ Policy gradient methods use calculus to optimize policy parameters</li>
                  <li>‚Ä¢ Value function approximation relies on gradient-based optimization</li>
                  <li>‚Ä¢ Temporal difference learning uses derivatives for error minimization</li>
                  <li>‚Ä¢ Continuous action spaces require calculus for policy optimization</li>
                </ul>
              </div>

              <div className="mt-8">
                <MathEquation 
                  equation="‚àáŒ∏ J(Œ∏) = E[‚àáŒ∏ log œÄ(a|s;Œ∏) Q(s,a)]" 
                  description="The policy gradient theorem, a fundamental result using calculus for policy optimization"
                />
              </div>

              <h3 className="text-xl font-semibold mt-8 mb-4">Gradient Descent Implementation</h3>
              <p className="mb-4 text-blue-100">
                Here's a practical implementation of gradient descent, a fundamental optimization algorithm used in many reinforcement learning methods:
              </p>
              <CodeBlock code={gradientDescentCode} language="python" />
            </div>
          </div>
        </div>

        {/* Probability Theory */}
        <SectionHeading 
          title="Probability Theory and Random Processes" 
          subtitle="The language of uncertainty and randomness in reinforcement learning."
        />

        <ContentBlock>
          <h3 className="text-xl font-semibold mb-4">Expected Value and Variance</h3>
          <p className="mb-4">
            Expected value (or expectation) is a fundamental concept in reinforcement learning, representing the
            long-term average value of a random variable. For a discrete random variable X with possible values
            x‚ÇÅ, x‚ÇÇ, ..., x‚Çô and corresponding probabilities p‚ÇÅ, p‚ÇÇ, ..., p‚Çô, the expected value is:
          </p>
          <MathEquation 
            equation="E[X] = ‚àë x_i * p_i" 
            description="Expected value is the sum of each possible value multiplied by its probability."
          />
          <p className="mb-4">
            Variance measures the spread or dispersion of a random variable around its expected value:
          </p>
          <MathEquation 
            equation="Var(X) = E[(X - E[X])¬≤] = E[X¬≤] - (E[X])¬≤" 
            description="Variance is the expected value of the squared deviation from the mean."
          />
          
          <h3 className="text-xl font-semibold mb-4 mt-8">Conditional Probability</h3>
          <p className="mb-4">
            Conditional probability is essential for reasoning about the likelihood of events given that other events
            have occurred. The conditional probability of event A given event B is:
          </p>
          <MathEquation 
            equation="P(A|B) = P(A ‚à© B) / P(B)" 
            description="The probability of A given B equals the probability of both A and B occurring, divided by the probability of B."
          />
          <p className="mb-4">
            In reinforcement learning, conditional probabilities are used to model state transitions, rewards, and
            the agent's observation of the environment.
          </p>
        </ContentBlock>

        {/* Markov Decision Processes */}
        <SectionHeading 
          title="Markov Decision Processes (MDPs)" 
          subtitle="The mathematical framework for sequential decision making."
        />

        <ContentBlock>
          <p className="mb-4">
            Markov Decision Processes (MDPs) provide the formal framework for most reinforcement learning problems.
            An MDP is defined by:
          </p>
          <ul className="list-disc pl-6 mb-6 space-y-2">
            <li><strong>S</strong>: A set of states</li>
            <li><strong>A</strong>: A set of actions</li>
            <li><strong>P(s'|s,a)</strong>: Transition probability function, giving the probability of transitioning to state s' from state s after taking action a</li>
            <li><strong>R(s,a,s')</strong>: Reward function, giving the expected immediate reward when transitioning from s to s' via action a</li>
            <li><strong>Œ≥</strong>: Discount factor, which determines the importance of future rewards (0 ‚â§ Œ≥ ‚â§ 1)</li>
          </ul>
          
          <h3 className="text-xl font-semibold mb-4">The Markov Property</h3>
          <p className="mb-4">
            The Markov property states that the future depends only on the present state, not on the sequence of
            events that preceded it:
          </p>
          <MathEquation 
            equation="P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1}|s_t, a_t)" 
            description="The probability of the next state depends only on the current state and action, not on the history."
          />
          
          <h3 className="text-xl font-semibold mb-4 mt-8">Policies and Value Functions</h3>
          <p className="mb-4">
            A policy œÄ is a mapping from states to probabilities of selecting each possible action. The value
            function V^œÄ(s) of a state s under policy œÄ is the expected return when starting in s and following
            policy œÄ thereafter:
          </p>
          <MathEquation 
            equation="V^œÄ(s) = E_œÄ[R_t | s_t = s] = E_œÄ[‚àë_{k=0}^‚àû Œ≥^k r_{t+k+1} | s_t = s]" 
            description="The state-value function is the expected sum of discounted rewards when starting from state s and following policy œÄ."
          />
          <p className="mb-4">
            Similarly, the action-value function Q^œÄ(s,a) is the expected return when starting in state s, taking
            action a, and following policy œÄ thereafter:
          </p>
          <MathEquation 
            equation="Q^œÄ(s,a) = E_œÄ[R_t | s_t = s, a_t = a] = E_œÄ[‚àë_{k=0}^‚àû Œ≥^k r_{t+k+1} | s_t = s, a_t = a]" 
            description="The action-value function is the expected sum of discounted rewards when starting from state s, taking action a, and following policy œÄ."
          />
        </ContentBlock>

        {/* Bellman Equations */}
        <SectionHeading 
          title="Bellman Equations and Optimality" 
          subtitle="The recursive relationships that define value functions."
        />

        <ContentBlock>
          <p className="mb-4">
            Bellman equations provide recursive relationships for value functions, expressing the value of a state
            in terms of the values of successor states. These equations are the foundation of many reinforcement
            learning algorithms.
          </p>
          
          <h3 className="text-xl font-semibold mb-4">Bellman Expectation Equation</h3>
          <p className="mb-4">
            For a given policy œÄ, the Bellman expectation equation for the state-value function is:
          </p>
          <MathEquation 
            equation="V^œÄ(s) = ‚àë_a œÄ(a|s) ‚àë_{s'} P(s'|s,a) [R(s,a,s') + Œ≥V^œÄ(s')]" 
            description="The value of a state equals the expected reward plus the discounted value of the next state."
          />
          <p className="mb-4">
            And for the action-value function:
          </p>
          <MathEquation 
            equation="Q^œÄ(s,a) = ‚àë_{s'} P(s'|s,a) [R(s,a,s') + Œ≥ ‚àë_{a'} œÄ(a'|s') Q^œÄ(s',a')]" 
            description="The value of a state-action pair equals the expected reward plus the discounted expected value of the next state-action pair."
          />
          
          <h3 className="text-xl font-semibold mb-4 mt-8">Bellman Optimality Equation</h3>
          <p className="mb-4">
            The Bellman optimality equations define the optimal value functions:
          </p>
          <MathEquation 
            equation="V^*(s) = max_a ‚àë_{s'} P(s'|s,a) [R(s,a,s') + Œ≥V^*(s')]" 
            description="The optimal value of a state is the maximum expected reward plus discounted value obtainable by any action."
          />
          <p className="mb-4">
            And for the optimal action-value function:
          </p>
          <MathEquation 
            equation="Q^*(s,a) = ‚àë_{s'} P(s'|s,a) [R(s,a,s') + Œ≥ max_{a'} Q^*(s',a')]" 
            description="The optimal value of a state-action pair is the expected reward plus the discounted maximum value of any action from the next state."
          />
        </ContentBlock>

        {/* Dynamic Programming */}
        <SectionHeading 
          title="Dynamic Programming" 
          subtitle="Solving MDPs through iterative computation of value functions."
        />

        <ContentBlock>
          <p className="mb-4">
            Dynamic programming (DP) provides a collection of algorithms for computing optimal policies given a
            perfect model of the environment as an MDP. While DP methods are limited by their assumptions, they
            provide the mathematical foundation for many reinforcement learning algorithms.
          </p>
          
          <h3 className="text-xl font-semibold mb-4">Policy Evaluation</h3>
          <p className="mb-4">
            Policy evaluation computes the state-value function V^œÄ for a given policy œÄ. It involves iteratively
            applying the Bellman expectation equation as an update rule:
          </p>
          <MathEquation 
            equation="V_{k+1}(s) = ‚àë_a œÄ(a|s) ‚àë_{s'} P(s'|s,a) [R(s,a,s') + Œ≥V_k(s')]" 
            description="Iterative update rule for computing the value function of a policy."
          />
          
          <h3 className="text-xl font-semibold mb-4 mt-8">Policy Improvement</h3>
          <p className="mb-4">
            Policy improvement generates a better policy œÄ' from policy œÄ by acting greedily with respect to V^œÄ:
          </p>
          <MathEquation 
            equation="œÄ'(s) = argmax_a ‚àë_{s'} P(s'|s,a) [R(s,a,s') + Œ≥V^œÄ(s')]" 
            description="Policy improvement by selecting actions that maximize expected return."
          />
          
          <h3 className="text-xl font-semibold mb-4 mt-8">Policy Iteration and Value Iteration</h3>
          <p className="mb-4">
            Policy iteration alternates between policy evaluation and policy improvement until convergence to the
            optimal policy. Value iteration combines these steps by updating each state's value with the maximum
            over all actions of the expected returns:
          </p>
          <MathEquation 
            equation="V_{k+1}(s) = max_a ‚àë_{s'} P(s'|s,a) [R(s,a,s') + Œ≥V_k(s')]" 
            description="Value iteration update rule for computing the optimal value function."
          />
        </ContentBlock>

        {/* Practice Problems */}
        <SectionHeading 
          title="Practice Problems" 
          subtitle="Test your understanding of the mathematical concepts."
        />

        <ContentBlock>
          <div className="space-y-6">
            <div className="p-4 bg-blue-50 border border-blue-200 rounded-lg">
              <h4 className="font-semibold mb-2">Problem 1: Expected Value Calculation</h4>
              <p className="mb-2">
                A reinforcement learning agent is in a simple environment with three possible rewards: +10 with
                probability 0.2, +5 with probability 0.5, and -3 with probability 0.3. Calculate the expected reward.
              </p>
              <details className="mt-2">
                <summary className="cursor-pointer text-blue-600 hover:text-blue-700">Show Solution</summary>
                <div className="mt-2 p-3 bg-white rounded">
                  <p>E[R] = (10 √ó 0.2) + (5 √ó 0.5) + (-3 √ó 0.3) = 2 + 2.5 - 0.9 = 3.6</p>
                </div>
              </details>
            </div>
            
            <div className="p-4 bg-blue-50 border border-blue-200 rounded-lg">
              <h4 className="font-semibold mb-2">Problem 2: Bellman Equation Application</h4>
              <p className="mb-2">
                Consider a simple MDP with two states (s‚ÇÅ and s‚ÇÇ) and two actions (a‚ÇÅ and a‚ÇÇ). The transition
                probabilities and rewards are as follows:
              </p>
              <ul className="list-disc pl-6 mb-2 space-y-1">
                <li>P(s‚ÇÅ|s‚ÇÅ,a‚ÇÅ) = 0.7, R(s‚ÇÅ,a‚ÇÅ,s‚ÇÅ) = 5</li>
                <li>P(s‚ÇÇ|s‚ÇÅ,a‚ÇÅ) = 0.3, R(s‚ÇÅ,a‚ÇÅ,s‚ÇÇ) = 10</li>
                <li>P(s‚ÇÅ|s‚ÇÅ,a‚ÇÇ) = 0.4, R(s‚ÇÅ,a‚ÇÇ,s‚ÇÅ) = 0</li>
                <li>P(s‚ÇÇ|s‚ÇÅ,a‚ÇÇ) = 0.6, R(s‚ÇÅ,a‚ÇÇ,s‚ÇÇ) = 15</li>
              </ul>
              <p className="mb-2">
                Given V(s‚ÇÇ) = 20 and a discount factor Œ≥ = 0.9, calculate Q(s‚ÇÅ,a‚ÇÅ) and Q(s‚ÇÅ,a‚ÇÇ).
              </p>
              <details className="mt-2">
                <summary className="cursor-pointer text-blue-600 hover:text-blue-700">Show Solution</summary>
                <div className="mt-2 p-3 bg-white rounded">
                  <p>Q(s‚ÇÅ,a‚ÇÅ) = P(s‚ÇÅ|s‚ÇÅ,a‚ÇÅ)[R(s‚ÇÅ,a‚ÇÅ,s‚ÇÅ) + Œ≥V(s‚ÇÅ)] + P(s‚ÇÇ|s‚ÇÅ,a‚ÇÅ)[R(s‚ÇÅ,a‚ÇÅ,s‚ÇÇ) + Œ≥V(s‚ÇÇ)]</p>
                  <p>Since V(s‚ÇÅ) isn't given, we can only compute the part involving s‚ÇÇ:</p>
                  <p>Q(s‚ÇÅ,a‚ÇÅ) = 0.7[5 + 0.9V(s‚ÇÅ)] + 0.3[10 + 0.9 √ó 20]</p>
                  <p>Q(s‚ÇÅ,a‚ÇÅ) = 0.7[5 + 0.9V(s‚ÇÅ)] + 0.3[10 + 18] = 0.7[5 + 0.9V(s‚ÇÅ)] + 0.3[28]</p>
                  <p>Q(s‚ÇÅ,a‚ÇÅ) = 3.5 + 0.63V(s‚ÇÅ) + 8.4 = 11.9 + 0.63V(s‚ÇÅ)</p>
                  <p>Similarly for Q(s‚ÇÅ,a‚ÇÇ):</p>
                  <p>Q(s‚ÇÅ,a‚ÇÇ) = 0.4[0 + 0.9V(s‚ÇÅ)] + 0.6[15 + 0.9 √ó 20] = 0.36V(s‚ÇÅ) + 0.6[15 + 18] = 0.36V(s‚ÇÅ) + 19.8</p>
                </div>
              </details>
            </div>
          </div>
        </ContentBlock>
      </div>
    </div>
  );
};

export default MathFoundationsPage;